{"cells":[{"cell_type":"code","source":["import requests\n","import time\n","import pandas as pd\n","from datetime import datetime\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from pyspark.sql import DataFrame"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"a9003428-e1d1-472e-af1a-f546f0f96fa0","normalized_state":"finished","queued_time":"2024-12-20T01:27:18.4369902Z","session_start_time":null,"execution_start_time":"2024-12-20T01:27:25.1585365Z","execution_finish_time":"2024-12-20T01:27:28.2588209Z","parent_msg_id":"7e1e5f11-daf5-4b09-b384-539ac4ee784b"},"text/plain":"StatementMeta(, a9003428-e1d1-472e-af1a-f546f0f96fa0, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"56f331ea-3e68-4c46-8771-5a253d3860b7"},{"cell_type":"code","source":["pbi_resource = \"https://analysis.windows.net/powerbi/api\"\n","pbi_Uri = 'https://api.powerbi.com/v1.0/myorg/'"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"a9003428-e1d1-472e-af1a-f546f0f96fa0","normalized_state":"finished","queued_time":"2024-12-20T01:27:18.5836339Z","session_start_time":null,"execution_start_time":"2024-12-20T01:27:28.7886112Z","execution_finish_time":"2024-12-20T01:27:29.1337117Z","parent_msg_id":"576899d5-368c-4c58-aa81-6919bf3b0973"},"text/plain":"StatementMeta(, a9003428-e1d1-472e-af1a-f546f0f96fa0, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cc6fb17a-ffac-42d4-8e5d-719758a8022d"},{"cell_type":"code","source":["def get_token():\n","    return mssparkutils.credentials.getToken(pbi_resource)\n","\n","def get_powerbiAPIclusterURI():\n","    fullurl = pbi_Uri+'datasets'\n","    pbi_access_token = get_token()\n","    headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {pbi_access_token}'}\n","    response = requests.get(fullurl, headers=headers)\n","    unaltered = response.json()['@odata.context']\n","    stripped = unaltered.split('/')\n","    return f'https://{stripped[2]}/beta/myorg/groups'\n","\n","clusteredURI = get_powerbiAPIclusterURI()\n","\n","def get_AccessibleWorkspaces():\n","    fullUrl = pbi_Uri+\"/groups?$filter=type eq 'Workspace'\"\n","    pbi_access_token = get_token()\n","    headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {pbi_access_token}'}\n","    response = requests.get(fullUrl, headers=headers)\n","    return response.json()['value']\n","\n","def get_WorkspaceUsageMetricsId(wsId):\n","    fullurl = f'{clusteredURI}/{wsId}/usageMetricsReportV2'\n","    # print('Asked for token')\n","    pbi_access_token = get_token()\n","    # print('Token received')\n","\n","    headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {pbi_access_token}'}\n","    attemps = 0\n","    while attemps < 4:\n","       # print('Starting ' + str(attemps))\n","        try:\n","            response = requests.get(fullurl, headers=headers, timeout=60)\n","        #    print('Received data')\n","            return response.json()['models'][0]['dbName']\n","        except:\n","        #    print('Fallin asleep')\n","            time.sleep(30)\n","        #    print('Awaken')\n","            attemps += 1\n","\n","def post_ExecuteQuery(wsId, dsId, daxQ):\n","    fullurl = f'{pbi_Uri}/groups/{wsId}/datasets/{dsId}/executeQueries'\n","    pbi_access_token = get_token()\n","    headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {pbi_access_token}'}\n","    content = {\"queries\": [{\"query\": daxQ}], \"serializerSettings\": {\"includeNulls\": True}}\n","    \n","    attempts = 0\n","    while attempts < 4:\n","        try:\n","            # Make the POST request\n","            response = requests.post(fullurl, json=content, headers=headers, timeout=120)\n","            \n","            # Check for HTTP errors\n","            response.raise_for_status()\n","            \n","            # Parse the JSON response\n","            data = response.json()\n","            \n","            # Validate the structure of the response\n","            if 'results' in data and len(data['results']) > 0 and 'tables' in data['results'][0]:\n","                return data['results'][0]['tables'][0]\n","            else:\n","                print(f\"Unexpected response structure: {data}\")\n","                return None\n","        \n","        except requests.exceptions.RequestException as e:\n","            print(f\"Request failed on attempt {attempts + 1}: {e}\")\n","            time.sleep(30)\n","            attempts += 1\n","        \n","        except KeyError as e:\n","            print(f\"Key error in response: {e}\")\n","            return None\n","    \n","    # If all attempts fail, return None\n","    print(f\"Failed to execute query after {attempts} attempts.\")\n","    return None\n","\n","def refresh_execute(wsId, dsId) :\n","    fullurl = f'{pbi_Uri}/groups/{wsId}/datasets/{dsId}/refreshes'\n","    pbi_access_token = get_token()\n","    headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {pbi_access_token}'}\n","    content = {\n","    \"type\": \"Full\",\n","    \"commitMode\": \"transactional\",\n","    \"maxParallelism\": 2,\n","    \"retryCount\": 2}\n","    response = requests.post(fullurl, json=content, headers=headers, timeout=120)\n","    return print(f\"Refresh status for WsId {wsId} DsId {dsId} is {response}.\")\n","\n","def replace_ColumnNames(frame, replacingName):\n","    return frame.toDF(*(c.replace(replacingName,'').replace('[','').replace(']','') for c in frame.columns))\n","\n","def extract_DataFrame(response_list, preCreatedSchema):\n","    rl = spark.createDataFrame(response_list)\n","    # print('Frame Created')\n","    # display(rl)\n","    rl = rl.select(explode(rl.rows)).select(\"col\").rdd.flatMap(lambda x: x).collect()\n","    #print('Columns Extracted')\n","    if not schema:\n","        rl = spark.createDataFrame(rl)\n","    else:\n","        rl = spark.createDataFrame(rl, preCreatedSchema)\n","    #print('Extraction Completed')\n","    rl=rl.withColumn(\"insert_date\", lit(current_date()))\n","    return rl\n","\n","def merge_workspace_data(existing_df: DataFrame, new_df: DataFrame) -> DataFrame:\n","    # Use Spark's distinct operation to ensure there are no duplicates\n","    merged_df = existing_df.union(new_df).dropDuplicates([\"id\"])  # Assuming \"id\" is the unique identifier for workspaces\n","    return merged_df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"a9003428-e1d1-472e-af1a-f546f0f96fa0","normalized_state":"finished","queued_time":"2024-12-20T01:27:18.6530865Z","session_start_time":null,"execution_start_time":"2024-12-20T01:27:29.7239998Z","execution_finish_time":"2024-12-20T01:27:30.7526522Z","parent_msg_id":"363ceafe-4098-41b5-9b93-bb4c16054806"},"text/plain":"StatementMeta(, a9003428-e1d1-472e-af1a-f546f0f96fa0, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7e05a259-4b4b-4c73-9ad5-59cec400b524"},{"cell_type":"code","source":["tblsInLK = spark.catalog.listTables()\n","tblsReady = len(tblsInLK)\n","if tblsReady == 0:\n","    doesTableUMDExists = 0\n","else:\n","    tblsInLK = spark.createDataFrame(pd.DataFrame(tblsInLK))\n","    doesTableUMDExists = tblsInLK.filter(col('name') == 'UsageMetricsDatasets').count()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"a9003428-e1d1-472e-af1a-f546f0f96fa0","normalized_state":"finished","queued_time":"2024-12-20T01:27:18.7386352Z","session_start_time":null,"execution_start_time":"2024-12-20T01:27:31.3138456Z","execution_finish_time":"2024-12-20T01:27:49.080444Z","parent_msg_id":"dbffc097-2e02-467b-a295-2ff3830b3271"},"text/plain":"StatementMeta(, a9003428-e1d1-472e-af1a-f546f0f96fa0, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"154edfa5-7c28-4259-8098-698422d1b5ce"},{"cell_type":"code","source":["wsList = get_AccessibleWorkspaces()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"a9003428-e1d1-472e-af1a-f546f0f96fa0","normalized_state":"finished","queued_time":"2024-12-20T01:27:18.8326097Z","session_start_time":null,"execution_start_time":"2024-12-20T01:27:49.6181217Z","execution_finish_time":"2024-12-20T01:27:49.970557Z","parent_msg_id":"19e4f7cd-52e7-4502-b357-7da55185d385"},"text/plain":"StatementMeta(, a9003428-e1d1-472e-af1a-f546f0f96fa0, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ffdd1ec9-59de-479d-83b3-9b38c52dafc2"},{"cell_type":"code","source":["# If the table does not exist, create it\n","if doesTableUMDExists == 0:\n","    spark.conf.set(\"spark.sql.caseSensitive\", \"true\")   \n","    # Add the DatasetId to each workspace entry\n","    wsList_with_DId = []\n","    for ws in wsList:\n","        wsId = ws['id']  \n","        dataset_id = get_WorkspaceUsageMetricsId(wsId)  # Fetch the DatasetId\n","        # Only add the workspace if the DatasetId is not None\n","        if dataset_id is not None:\n","            ws['DatasetId'] = dataset_id \n","            wsList_with_DId.append(ws)\n","    \n","    # Convert the enriched wsList (with DatasetId) into a Spark DataFrame\n","    wsList_df = spark.createDataFrame(wsList_with_DId)\n","    # Add an 'insert_date' column with the current date\n","    wsList_df = wsList_df.withColumn(\"insert_date\", lit(current_date()))\n","    wsList_df = wsList_df.withColumn(\"ExtractionState\", lit(0))\n","    # Create the table in the Lakehouse\n","    wsList_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"UsageMetricsDatasets\")\n","    print(\"Table 'UsageMetricsDatasets' created successfully with DatasetId column.\")\n","else:\n","    spark.conf.set(\"spark.sql.caseSensitive\", \"true\")\n","    existing_UMD = spark.table(\"UsageMetricsDatasets\")\n","    \n","    # Get the list of workspace IDs that already exist in the table\n","    existing_workspace_ids = existing_UMD.select(\"id\").rdd.flatMap(lambda x: x).collect()\n","\n","    # Fetch new workspaces and filter out those that already exist in the table\n","    new_UMD = []\n","    for ws in wsList:\n","        wsId = ws['id']\n","\n","        # Only fetch the DatasetId for workspaces that don't exist in the table\n","        if wsId not in existing_workspace_ids:\n","            dataset_id = get_WorkspaceUsageMetricsId(wsId)\n","            \n","            # Only include workspaces with a non-None DatasetId\n","            if dataset_id is not None:\n","                ws['DatasetId'] = dataset_id\n","                new_UMD.append(ws)\n","\n","    # If there are new workspaces to process\n","    if new_UMD:\n","        # Convert the new data to a DataFrame\n","        new_UMD_df = spark.createDataFrame(new_UMD)\n","        new_UMD_df = new_UMD_df.withColumn(\"insert_date\", lit(current_date()))\n","        new_UMD_df = new_UMD_df.withColumn(\"ExtractionState\", lit(0))\n","        # Merge the new workspace data with the existing table data\n","        merged_df = merge_workspace_data(existing_UMD, new_UMD_df)\n","        # Overwrite the existing table with the merged data\n","        merged_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"UsageMetricsDatasets\")\n","\n","        print(\"Table 'UsageMetricsDatasets' updated with new workspace data and DatasetId.\")\n","    else:\n","        print(\"No new workspaces to update.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"a9003428-e1d1-472e-af1a-f546f0f96fa0","normalized_state":"finished","queued_time":"2024-12-20T01:27:18.9058458Z","session_start_time":null,"execution_start_time":"2024-12-20T01:27:50.5253054Z","execution_finish_time":"2024-12-20T01:30:50.1680675Z","parent_msg_id":"3b9ee5ea-3c61-4d4b-aa9b-06b9cbda36be"},"text/plain":"StatementMeta(, a9003428-e1d1-472e-af1a-f546f0f96fa0, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Table 'UsageMetricsDatasets' updated with new workspace data and DatasetId.\n"]}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c53cb0e9-ccb3-4661-9691-467093c2d272"},{"cell_type":"code","source":["updated_UMD = spark.table(\"UsageMetricsDatasets\")\n","updated_UMD = updated_UMD.filter(col(\"id\").isNotNull() & col(\"DatasetId\").isNotNull())\n","updated_UMD = updated_UMD.select(\"id\", \"DatasetId\",\"ExtractionState\").rdd.map(lambda row: row.asDict()).collect()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"a9003428-e1d1-472e-af1a-f546f0f96fa0","normalized_state":"finished","queued_time":"2024-12-20T01:27:18.986626Z","session_start_time":null,"execution_start_time":"2024-12-20T01:30:50.6613813Z","execution_finish_time":"2024-12-20T01:30:53.4765599Z","parent_msg_id":"72aaad23-6bb9-463b-9692-744c3378cfda"},"text/plain":"StatementMeta(, a9003428-e1d1-472e-af1a-f546f0f96fa0, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"21199459-34c0-4c68-a8e1-46ddcc4459e2"},{"cell_type":"code","source":["for ws in updated_UMD:\n","    dsId = ws['DatasetId'] \n","    wsId = ws['id']\n","    response = refresh_execute(wsId,dsId)\n","    time.sleep(0.5)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f5308f25-2b36-4ec3-983e-b332d11eb2c3"},{"cell_type":"code","source":["basicListOfTables = ['Reports','Users','Report pages','Workspace views'] # List of all basic tables from that need to be extracted\n","for bsTbl in basicListOfTables:\n","    response_list = []\n","    if bsTbl == \"Reports\":\n","        schema = \\\n","            StructType([ \n","                StructField(f\"{bsTbl}[IsUsageMetricsReport]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[OrganizationId]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[ReportGuid]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[ReportName]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[WorkspaceId]\", StringType(), True), \\\n","            ])\n","    elif bsTbl == \"Users\":\n","        schema = \\\n","            StructType([ \n","                StructField(f\"{bsTbl}[UniqueUser]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[UserGuid]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[UserId]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[UserKey]\",StringType(),True), \\\n","            ])\n","    elif bsTbl == \"Report pages\":\n","        schema = \\\n","            StructType([ \n","                StructField(f\"{bsTbl}[ReportId]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[SectionId]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[SectionName]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[WorkspaceId]\", StringType(), True), \\\n","            ])\n","    elif bsTbl == \"Workspace views\":\n","        schema = \\\n","            StructType([ \n","                StructField(f\"{bsTbl}[ConsumptionMethod]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[DistributionMethod]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[ReportId]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[UniqueUser]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[UserId]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[UserKey]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[Views]\", StringType(), True), \\\n","            ])           \n","    else:\n","        schema = None    \n","    for ws in updated_UMD:\n","        dsId = ws['DatasetId'] \n","        wsId = ws['id']\n","        response = post_ExecuteQuery(wsId,dsId,f\"EVALUATE '{bsTbl}'\")\n","        size = len(response['rows'])\n","        if size != 0:\n","            response_list.append(response)\n","        time.sleep(0.5)\n","    df = extract_DataFrame(response_list,schema)\n","    df = replace_ColumnNames(df,bsTbl)\n","    updatedText = bsTbl.replace(\"'\",\"\").title().replace(\" \",\"\")\n","    if bsTbl == 'Users':\n","        df = df.distinct()    \n","    writetolake = df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{updatedText}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"a9003428-e1d1-472e-af1a-f546f0f96fa0","normalized_state":"finished","queued_time":"2024-12-20T01:27:19.2222876Z","session_start_time":null,"execution_start_time":"2024-12-20T01:31:45.4859937Z","execution_finish_time":"2024-12-20T01:34:19.0803359Z","parent_msg_id":"a3a683e0-4e4c-4849-b042-51d1cef69eb9"},"text/plain":"StatementMeta(, a9003428-e1d1-472e-af1a-f546f0f96fa0, 11, Finished, Available, Finished)"},"metadata":{}}],"execution_count":9,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fea019cb-13d8-4bdd-a27a-d49d99d18f9c"},{"cell_type":"code","source":["basicListOfTables = [\"Report page views\",\"Report load times\",\"Report views\"] # List of all basic tables from that need to be extracted\n","md = \"\"\n","dax = \"\"\n","for bsTbl in basicListOfTables:\n","    findTable = \"\"\n","    updatedText = bsTbl.replace(\"'\",\"\").title().replace(\" \",\"\")\n","    if tblsReady == 0:\n","        doesTableExists = 0\n","    else:\n","        findTable = tblsInLK.filter(col('name') == updatedText)\n","        doesTableExists = findTable.count()\n","    response_list = []\n","    if bsTbl == \"Report page views\":\n","        schema = \\\n","            StructType([ \n","                StructField(f\"{bsTbl}[UserKey]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[UserId]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[TenantId]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[Timestamp]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[AppGuid]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[SectionId]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[AppName]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[Date]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[DeviceBrowserVersion]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[ReportId]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[SessionSource]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[OriginalReportId]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[WorkspaceId]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[Client]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[OriginalWorkspaceId]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[DeviceOSVersion]\", StringType(), True) \\\n","            ])\n","    elif bsTbl == \"Report load times\":\n","        schema = \\\n","            StructType([ \n","                StructField(f\"{bsTbl}[DeviceOSVersion]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[AppGuid]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[Timestamp]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[UserId]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[loadTime]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[Client]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[DeviceBrowserVersion]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[OriginalGroupId]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[TenantId]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[EndTime]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[SessionSource]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[AppName]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[Browser]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[OriginalReportId]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[GroupId]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[StartTime]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[Date]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[LocationCity]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[Country]\", StringType(), True) \\\n","            ])\n","    elif bsTbl == \"Report views\":\n","        schema = \\\n","            StructType([ \n","                StructField(f\"{bsTbl}[AppName]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[CapacityId]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[CapacityName]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[ConsumptionMethod]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[CreationTime]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[DatasetName]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[Date]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[DistributionMethod]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[OriginalConsumptionMethod]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[ReportId]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[ReportName]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[ReportType]\", StringType(), True),  \\\n","                StructField(f\"{bsTbl}[UserAgent]\", StringType(), True), \\\n","                StructField(f\"{bsTbl}[UserId]\",StringType(),True), \\\n","                StructField(f\"{bsTbl}[UserKey]\",StringType(),True) \\\n","            ])\n","    else:\n","        schema = None\n","    for ws in updated_UMD:\n","        dsId = ws['DatasetId'] \n","        wsId = ws['id']\n","        dsExportState = ws['ExtractionState']\n","        if doesTableExists == 0:\n","            dax = f\"EVALUATE '{bsTbl}'\"\n","            md = \"overwrite\"\n","        else:\n","            tableLastDate = spark.sql(f\"SELECT MAX(Date) AS date FROM {updatedText}\").collect()[0]['date']\n","            fromDate = tableLastDate.strftime('DATE(%Y,%m,%d)')\n","            dax = f\"DEFINE VAR _td = TODAY() VAR _from = {fromDate} EVALUATE FILTER('{bsTbl}',[Date]>_from && [Date]<_td)\"\n","            md = \"append\"\n","\n","        response = post_ExecuteQuery(wsId, dsId, dax)\n","        if response and 'rows' in response:\n","            size = len(response['rows'])\n","            if size != 0:\n","                response_list.append(response)\n","        time.sleep(0.5)\n","\n","    if response_list:\n","        # Extract data into DataFrame\n","        exports = extract_DataFrame(response_list, schema)\n","        # Convert 'Date' field to TimestampType after creating the DataFrame\n","        exports = exports.withColumn(f\"{bsTbl}[Date]\", to_timestamp(col(f\"{bsTbl}[Date]\")))\n","        # Replace column names and write to Delta\n","        exports = replace_ColumnNames(exports, bsTbl)\n","        writetolake = exports.write.mode(md).format(\"delta\").save(f\"Tables/{updatedText}\")\n","    else:\n","        table_df = spark.read.format(\"delta\").load(f\"Tables/{updatedText}\")\n","        current_date = to_date(lit(datetime.now().strftime('%Y-%m-%d')), 'yyyy-MM-dd')\n","        updated_table_df = table_df.withColumn(\"insert_date\", current_date)\n","        updated_table_df.write.format(\"delta\").mode(\"overwrite\").save(f\"Tables/{updatedText}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"a9003428-e1d1-472e-af1a-f546f0f96fa0","normalized_state":"finished","queued_time":"2024-12-20T01:27:19.2283139Z","session_start_time":null,"execution_start_time":"2024-12-20T01:34:19.5412346Z","execution_finish_time":"2024-12-20T01:36:57.2548022Z","parent_msg_id":"ffe5a3ff-b4a1-4b70-8b19-b97a8c5c3495"},"text/plain":"StatementMeta(, a9003428-e1d1-472e-af1a-f546f0f96fa0, 12, Finished, Available, Finished)"},"metadata":{}}],"execution_count":10,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"089c7f9b-07a7-4e70-91f1-98f6d4d49e45"},{"cell_type":"code","source":["# #Only run this code if you need to replace old report id with new id (same report)\n","# df = spark.read.format(\"delta\").load(\"\")\n","\n","# updated_df = df.withColumn(\n","#     \"ReportId\",\n","#     when(col(\"ReportId\") == \"\", \"\")\n","#     .otherwise(col(\"ReportId\"))\n","# )\n","# updated_df.write.format(\"delta\").mode(\"overwrite\").save(\"\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"a9003428-e1d1-472e-af1a-f546f0f96fa0","normalized_state":"finished","queued_time":"2024-12-20T01:27:19.3096635Z","session_start_time":null,"execution_start_time":"2024-12-20T01:36:57.77331Z","execution_finish_time":"2024-12-20T01:36:58.1246148Z","parent_msg_id":"b5ddc972-323b-41b3-bdd4-45cabdb74e3b"},"text/plain":"StatementMeta(, a9003428-e1d1-472e-af1a-f546f0f96fa0, 13, Finished, Available, Finished)"},"metadata":{}}],"execution_count":11,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e4da8cdf-72d6-4113-bbfc-0687c1f2a5a9"},{"cell_type":"code","source":["# #Only run this code if you just reset usage metric semantic model\n","# df = spark.read.format(\"delta\").load(\"\")\n","\n","# # Filter out the row to delete\n","# filtered_df = df.filter(col(\"id\") != \"12d07675-c0f1-461c-a09c-d6d3e351f39c\")\n","\n","# # Overwrite the table with the updated DataFrame\n","# filtered_df.write.format(\"delta\").mode(\"overwrite\").save(\"\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"a9003428-e1d1-472e-af1a-f546f0f96fa0","normalized_state":"finished","queued_time":"2024-12-20T01:27:19.3759649Z","session_start_time":null,"execution_start_time":"2024-12-20T01:36:58.7095253Z","execution_finish_time":"2024-12-20T01:36:59.0598251Z","parent_msg_id":"a976c00c-88f8-464d-917e-1086877cbd19"},"text/plain":"StatementMeta(, a9003428-e1d1-472e-af1a-f546f0f96fa0, 14, Finished, Available, Finished)"},"metadata":{}}],"execution_count":12,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"20dba03c-afb3-442a-8aa1-593ce030628e"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"f3c9ded9-6c89-4ba5-ae11-3187e4111b37","default_lakehouse_name":"Usage_metric","default_lakehouse_workspace_id":"e51835b4-bcdc-486a-979d-3dc8ff188748"}}},"nbformat":4,"nbformat_minor":5}